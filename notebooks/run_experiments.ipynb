{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReID Experiments Evaluation\n",
    "\n",
    "This notebook evaluates saved ReID models using configurations from `experiments.yaml`.\n",
    "It supports flexible checkpoint selection (final_model.pth, best_model.pth, or specific epoch checkpoints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/master-thesis-reid\n",
    "    !pip install -q torchreid matplotlib seaborn scipy\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "if IN_COLAB:\n",
    "    project_root = Path('/content/master-thesis-reid')\n",
    "else:\n",
    "    project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import get_dataloaders_from_config\n",
    "from models.person import *\n",
    "from models.vehicle import *\n",
    "import torchreid\n",
    "from torchreid import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = project_root / 'config'\n",
    "\n",
    "\n",
    "with open(config_dir / 'experiments.yaml') as f:\n",
    "    exp_config = yaml.safe_load(f)\n",
    "    print(\"Loaded experiments.yaml\")\n",
    "\n",
    "\n",
    "with open(config_dir / 'train_experiments.yaml') as f:\n",
    "    train_config = yaml.safe_load(f)\n",
    "    print(\"Loaded train_experiments.yaml\")\n",
    "\n",
    "\n",
    "MODEL_ROOT = exp_config['global_methodology']['model_root']\n",
    "DATA_ROOT = exp_config['global_methodology']['data_root']\n",
    "RESULTS_DIR = exp_config['global_methodology']['results_dir']\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_DIR, 'visualizations'), exist_ok=True)\n",
    "\n",
    "print(f\"\\nModel root: {MODEL_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_type(model_name):\n",
    "    \"\"\"Determine if model is for person or vehicle ReID.\"\"\"\n",
    "    vehicle_models = ['resnet50_vehicle', 'aaver', 'rptm', 'vat']\n",
    "    return 'vehicle' if model_name in vehicle_models else 'person'\n",
    "\n",
    "def build_model(model_name, num_classes, loss='softmax', model_type='person'):\n",
    "    \"\"\"Build a ReID model.\"\"\"\n",
    "    \n",
    "    if model_type == 'person':\n",
    "        if model_name == 'osnet_x1_0':\n",
    "            return osnet_x1_0(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name == 'pcb_p6':\n",
    "            return pcb_p6(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name == 'pcb_p4':\n",
    "            return pcb_p4(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name == 'hacnn':\n",
    "            return hacnn(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name in ['transreid_base', 'transreid']:\n",
    "            return transreid_base(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name == 'autoreid_plus':\n",
    "            return autoreid_plus(num_classes=num_classes, loss=model_loss)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown person ReID model: {model_name}\")\n",
    "    elif model_type == 'vehicle':\n",
    "        if model_name == 'resnet50_vehicle':\n",
    "            return resnet50_vehicle(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name == 'aaver':\n",
    "            return aaver(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name == 'rptm':\n",
    "            return rptm(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        elif model_name == 'vat':\n",
    "            return vat(num_classes=num_classes, loss=model_loss, pretrained=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown vehicle ReID model: {model_name}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "def extract_features(model, dataloader, device):\n",
    "    \"\"\"Extract features from dataloader.\"\"\"\n",
    "    features_list, pids_list, camids_list, img_paths_list, times = [], [], [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, pids, camids, img_paths in tqdm(dataloader, desc='Extracting', leave=False):\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "            \n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                feats = outputs[0]\n",
    "            else:\n",
    "                feats = outputs\n",
    "            \n",
    "            feats = F.normalize(feats, p=2, dim=1)\n",
    "            features_list.append(feats.cpu())\n",
    "            pids_list.append(pids)\n",
    "            camids_list.append(camids)\n",
    "            img_paths_list.extend(img_paths)\n",
    "    \n",
    "    return (\n",
    "        torch.cat(features_list).numpy(),\n",
    "        torch.cat(pids_list).numpy(),\n",
    "        torch.cat(camids_list).numpy(),\n",
    "        img_paths_list,\n",
    "        times\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Experiments to Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiments = exp_config.get('experiment_evaluations', {})\n",
    "\n",
    "enabled_experiments = {name: cfg for name, cfg in experiments.items() if cfg.get('enabled', True)}\n",
    "\n",
    "print(f\"Total experiments to run: {len(enabled_experiments)}\\n\")\n",
    "for exp_name, exp_cfg in enabled_experiments.items():\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Dataset: {exp_cfg['dataset']}, K={exp_cfg['k_shot']}, Data: {exp_cfg['data_type']}\")\n",
    "    print(f\"  Models ({len(exp_cfg['models'])}):\")\n",
    "    for model in exp_cfg['models']:\n",
    "        print(f\"    - {model['name']}: {model['model']} ({model['loss']}, {model['checkpoint']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_results = []\n",
    "all_error_data = {}  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for exp_idx, (exp_name, exp_cfg) in enumerate(enabled_experiments.items(), 1):\n",
    "    if not exp_cfg.get('enabled', True):\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENT {exp_idx}/{len(enabled_experiments)}: {exp_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Dataset: {exp_cfg['dataset']}, K-shot: {exp_cfg['k_shot']}, Data type: {exp_cfg['data_type']}\")\n",
    "    print(f\"Models to evaluate: {len(exp_cfg['models'])}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    experiment_results = []\n",
    "    \n",
    "    dataset_suffix = '_preprocessed'\n",
    "    \n",
    "    dataset_name_with_suffix = exp_cfg['dataset'] + dataset_suffix\n",
    "    \n",
    "    for model_idx, model_cfg in enumerate(exp_cfg['models'], 1):\n",
    "        model_type = get_model_type(model_cfg['model'])\n",
    "        \n",
    "        print(f\"\\n[{model_idx}/{len(exp_cfg['models'])}] Evaluating: {model_cfg['name']}\")\n",
    "        print(f\"  Model: {model_cfg['model']}, Loss: {model_cfg['loss']}, Checkpoint: {model_cfg['checkpoint']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            if model_type == 'person':\n",
    "                model_train_cfg = train_config['person_reid_experiments'].get(model_cfg['model'])\n",
    "            else:\n",
    "                model_train_cfg = train_config['vehicle_reid_experiments'].get(model_cfg['model'])\n",
    "            \n",
    "            if model_train_cfg:\n",
    "                img_height = model_train_cfg['training'].get('img_height', 256)\n",
    "                img_width = model_train_cfg['training'].get('img_width', 128)\n",
    "            else:\n",
    "                img_height, img_width = 256, 128\n",
    "            \n",
    "            print(f\"  [1/7] Loading dataset...\")\n",
    "            \n",
    "            dataloaders = get_dataloaders_from_config(\n",
    "                root=DATA_ROOT,\n",
    "                dataset_name=dataset_name_with_suffix,\n",
    "                config_dir=str(project_root / 'config'),\n",
    "                k_shot=exp_cfg['k_shot'],\n",
    "                override_params={'height': img_height, 'width': img_width}\n",
    "            )\n",
    "            \n",
    "            query_loader = dataloaders['query']\n",
    "            gallery_loader = dataloaders['gallery']\n",
    "            \n",
    "            print(f\"  Query: {len(query_loader.dataset)}, Gallery: {len(gallery_loader.dataset)}\")\n",
    "            \n",
    "            print(f\"  [2/7] Loading model...\")\n",
    "            \n",
    "            \n",
    "            loss_suffix = model_cfg['loss']\n",
    "            train_dataset = model_cfg.get('train_dataset', exp_cfg['dataset'])\n",
    "            model_save_name = f\"{model_cfg['model']}_{train_dataset}_{exp_cfg['data_type']}_k{exp_cfg['k_shot']}_l{loss_suffix}\"\n",
    "            model_path = os.path.join(MODEL_ROOT, model_save_name, model_cfg['checkpoint'])\n",
    "\n",
    "            \n",
    "            if 'train_dataset' in model_cfg:\n",
    "                print(f\"  Cross-domain: Trained on {train_dataset}, testing on {exp_cfg['dataset']}\")\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"  ERROR: Model not found at {model_path}\")\n",
    "                continue\n",
    "            \n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            num_classes = checkpoint['num_classes']\n",
    "            \n",
    "            model = build_model(model_cfg['model'], num_classes, loss=model_cfg['loss'], model_type=model_type)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"  Loaded from epoch {checkpoint['epoch']}\")\n",
    "            \n",
    "            print(f\"  [3/7] Extracting features...\")\n",
    "            \n",
    "            qf, qp, qc, q_paths, qt = extract_features(model, query_loader, device)\n",
    "            gf, gp, gc, g_paths, gt = extract_features(model, gallery_loader, device)\n",
    "            \n",
    "            \n",
    "            total_time = sum(qt)\n",
    "            fps = len(qf) / total_time if total_time > 0 else 0\n",
    "            avg_time_ms = (total_time / len(qf)) * 1000 if len(qf) > 0 else 0\n",
    "            \n",
    "            print(f\"  [4/7] Computing distances...\")\n",
    "            \n",
    "            qf_tensor = torch.from_numpy(qf)\n",
    "            gf_tensor = torch.from_numpy(gf)\n",
    "            \n",
    "            distmat = metrics.compute_distance_matrix(qf_tensor, gf_tensor, metric='euclidean').numpy()\n",
    "            distmat_qq = metrics.compute_distance_matrix(qf_tensor, qf_tensor, metric='euclidean').numpy()\n",
    "            distmat_gg = metrics.compute_distance_matrix(gf_tensor, gf_tensor, metric='euclidean').numpy()\n",
    "            \n",
    "            print(f\"  [5/7] Re-ranking...\")\n",
    "            distmat_reranked = torchreid.utils.re_ranking(distmat, distmat_qq, distmat_gg) if exp_cfg.get('reranking', True) else distmat\n",
    "            \n",
    "            print(f\"  [6/7] Evaluating metrics...\")\n",
    "            cmc, mAP = metrics.evaluate_rank(distmat_reranked, qp, gp, qc, gc, use_metric_cuhk03=False)\n",
    "            \n",
    "            print(f\"  [7/7] Analyzing errors...\")\n",
    "            \n",
    "            hard_queries = []\n",
    "            fp_count = 0\n",
    "            fn_top10_count = 0\n",
    "            \n",
    "            for q_idx in range(len(qp)):\n",
    "                ranked_indices = np.argsort(distmat_reranked[q_idx])\n",
    "                ranked_pids = gp[ranked_indices]\n",
    "                \n",
    "                correct_mask = ranked_pids == qp[q_idx]\n",
    "                if correct_mask.any():\n",
    "                    first_correct_rank = np.where(correct_mask)[0][0]\n",
    "                else:\n",
    "                    first_correct_rank = len(gp)\n",
    "                \n",
    "                hard_queries.append({\n",
    "                    'query_idx': q_idx,\n",
    "                    'pid': qp[q_idx],\n",
    "                    'cam': qc[q_idx],\n",
    "                    'first_correct_rank': first_correct_rank,\n",
    "                    'img_path': q_paths[q_idx]\n",
    "                })\n",
    "                \n",
    "                \n",
    "                if ranked_pids[0] != qp[q_idx]:\n",
    "                    fp_count += 1\n",
    "                \n",
    "                \n",
    "                if qp[q_idx] not in ranked_pids[:10]:\n",
    "                    fn_top10_count += 1\n",
    "            \n",
    "            hard_queries = sorted(hard_queries, key=lambda x: x['first_correct_rank'], reverse=True)\n",
    "            \n",
    "            fp_rate = fp_count / len(qp)\n",
    "            fn_rate = fn_top10_count / len(qp)\n",
    "            \n",
    "            print(f\"\\n  RESULTS:\")\n",
    "            print(f\"    mAP={mAP:.2%}, R-1={cmc[0]:.2%}, R-5={cmc[4]:.2%}, R-10={cmc[9]:.2%}\")\n",
    "            print(f\"    FPS={fps:.1f}, Avg time={avg_time_ms:.1f}ms\")\n",
    "            print(f\"    False Positive Rate (Rank-1): {fp_rate:.2%} ({fp_count}/{len(qp)})\")\n",
    "            print(f\"    False Negative Rate (Top-10): {fn_rate:.2%} ({fn_top10_count}/{len(qp)})\")\n",
    "            print(f\"    Hardest query: Rank {hard_queries[0]['first_correct_rank']+1} (PID={hard_queries[0]['pid']}, Cam={hard_queries[0]['cam']})\")\n",
    "            \n",
    "            \n",
    "            results = {\n",
    "                'experiment_name': exp_name,\n",
    "                'model_name': model_cfg['name'],\n",
    "                'model': model_cfg['model'],\n",
    "                'model_type': model_type,\n",
    "                'dataset': exp_cfg['dataset'],\n",
    "                'data_type': exp_cfg['data_type'],\n",
    "                'k_shot': exp_cfg['k_shot'],\n",
    "                'checkpoint': model_cfg['checkpoint'],\n",
    "                'loss': model_cfg['loss'],\n",
    "                'mAP': float(mAP),\n",
    "                'rank1': float(cmc[0]),\n",
    "                'rank5': float(cmc[4]),\n",
    "                'rank10': float(cmc[9]),\n",
    "                'rank20': float(cmc[19]),\n",
    "                'fps': float(fps),\n",
    "                'avg_query_time_ms': float(avg_time_ms),\n",
    "                'train_epoch': checkpoint['epoch'],\n",
    "                'false_positive_rate': float(fp_rate),\n",
    "                'false_negative_top10_rate': float(fn_rate),\n",
    "                'hardest_query_rank': int(hard_queries[0]['first_correct_rank'])\n",
    "            }\n",
    "            \n",
    "            experiment_results.append(results)\n",
    "            all_results.append(results)\n",
    "            \n",
    "            error_key = f\"{exp_name}_{model_cfg['name']}\"\n",
    "            all_error_data[error_key] = {\n",
    "                'hard_queries': hard_queries[:50],  \n",
    "                'qp': qp,\n",
    "                'qc': qc,\n",
    "                'gp': gp,\n",
    "                'gc': gc,\n",
    "                'q_paths': q_paths,\n",
    "                'g_paths': g_paths,\n",
    "                'distmat': distmat_reranked,\n",
    "                'model_name': model_cfg['name'],\n",
    "                'experiment': exp_name\n",
    "            }\n",
    "            \n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ERROR: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    if experiment_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EXPERIMENT SUMMARY: {exp_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'Model':<20} {'mAP':<8} {'R-1':<8} {'R-5':<8} {'FP Rate':<10} {'FN Rate':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for r in experiment_results:\n",
    "            print(f\"{r['model_name']:<20} {r['mAP']*100:<8.2f} {r['rank1']*100:<8.2f} \"\n",
    "                  f\"{r['rank5']*100:<8.2f} {r['false_positive_rate']*100:<10.2f} {r['false_negative_top10_rate']*100:<10.2f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    \n",
    "    combined_path = os.path.join(RESULTS_DIR, 'experiment_results_complete.json')\n",
    "    with open(combined_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    print(f\"Saved: {combined_path}\")\n",
    "    \n",
    "    \n",
    "    csv_path = os.path.join(RESULTS_DIR, 'experiment_results_complete.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "    \n",
    "    \n",
    "    error_path = os.path.join(RESULTS_DIR, 'error_analysis_data.json')\n",
    "    \n",
    "    error_data_serializable = {}\n",
    "    for key, data in all_error_data.items():\n",
    "        error_data_serializable[key] = {\n",
    "            'hard_queries': data['hard_queries'],\n",
    "            'model_name': data['model_name'],\n",
    "            'experiment': data['experiment']\n",
    "        }\n",
    "    with open(error_path, 'w') as f:\n",
    "        json.dump(error_data_serializable, f, indent=2)\n",
    "    print(f\"Saved: {error_path}\")\n",
    "else:\n",
    "    print(\"No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE EXPERIMENT SUMMARY FOR THESIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. BEST PERFORMING MODELS\")\n",
    "    print(\"-\"*80)\n",
    "    person_best = df[df['dataset'] == 'market1501'].nlargest(3, 'mAP')\n",
    "    print(\"\\nPerson ReID (Market-1501):\")\n",
    "    for i, (_, row) in enumerate(person_best.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['model_name']}: mAP={row['mAP']:.2%}, R-1={row['rank1']:.2%} (K={row['k_shot']}, {row['data_type']})\")\n",
    "    \n",
    "    vehicle_best = df[df['dataset'] == 'veri776'].nlargest(3, 'mAP')\n",
    "    print(\"\\nVehicle ReID (VeRi-776):\")\n",
    "    for i, (_, row) in enumerate(vehicle_best.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['model_name']}: mAP={row['mAP']:.2%}, R-1={row['rank1']:.2%} (K={row['k_shot']}, {row['data_type']})\")\n",
    "    \n",
    "    print(\"\\n2. K-SHOT ANALYSIS SUMMARY\")\n",
    "    print(\"-\"*80)\n",
    "    person_kshot = df[(df['model'] == 'osnet_x1_0') & (df['dataset'] == 'market1501')].sort_values('k_shot')\n",
    "    if len(person_kshot) > 0:\n",
    "        print(\"\\nPerson ReID (OSNet):\")\n",
    "        for _, row in person_kshot.iterrows():\n",
    "            print(f\"  K={row['k_shot']:2d}: mAP={row['mAP']:.2%}, R-1={row['rank1']:.2%}\")\n",
    "    \n",
    "    vehicle_kshot = df[(df['model'] == 'rptm') & (df['dataset'] == 'veri776')].sort_values('k_shot')\n",
    "    if len(vehicle_kshot) > 0:\n",
    "        print(\"\\nVehicle ReID (RPTM):\")\n",
    "        for _, row in vehicle_kshot.iterrows():\n",
    "            print(f\"  K={row['k_shot']:2d}: mAP={row['mAP']:.2%}, R-1={row['rank1']:.2%}\")\n",
    "    \n",
    "    print(\"\\n3. ERROR ANALYSIS SUMMARY\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"\\nAverage False Positive Rates (Rank-1 errors):\")\n",
    "    person_fp = df[df['dataset'] == 'market1501']['false_positive_rate'].mean()\n",
    "    vehicle_fp = df[df['dataset'] == 'veri776']['false_positive_rate'].mean()\n",
    "    print(f\"  Person ReID: {person_fp:.2%}\")\n",
    "    print(f\"  Vehicle ReID: {vehicle_fp:.2%}\")\n",
    "    \n",
    "    print(\"\\nAverage False Negative Rates (Top-10 misses):\")\n",
    "    person_fn = df[df['dataset'] == 'market1501']['false_negative_top10_rate'].mean()\n",
    "    vehicle_fn = df[df['dataset'] == 'veri776']['false_negative_top10_rate'].mean()\n",
    "    print(f\"  Person ReID: {person_fn:.2%}\")\n",
    "    print(f\"  Vehicle ReID: {vehicle_fn:.2%}\")\n",
    "    \n",
    "    print(\"\\n4. INFERENCE SPEED SUMMARY\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"\\nFastest models (Person ReID):\")\n",
    "    person_fast = df[df['dataset'] == 'market1501'].nlargest(3, 'fps')\n",
    "    for i, (_, row) in enumerate(person_fast.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['model_name']}: {row['fps']:.1f} FPS ({row['avg_query_time_ms']:.1f}ms per query)\")\n",
    "    \n",
    "    print(\"\\nFastest models (Vehicle ReID):\")\n",
    "    vehicle_fast = df[df['dataset'] == 'veri776'].nlargest(3, 'fps')\n",
    "    for i, (_, row) in enumerate(vehicle_fast.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['model_name']}: {row['fps']:.1f} FPS ({row['avg_query_time_ms']:.1f}ms per query)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Report complete. All visualizations saved to:\", os.path.join(RESULTS_DIR, 'visualizations'))\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
