{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SSL training loop</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd master-thesis-reid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torchreid\n",
    "from torchreid import metrics\n",
    "\n",
    "project_root = Path('/content/master-thesis-reid')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from models.person import transreid_base\n",
    "from utils.data_loader import get_dataloaders_from_config\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchreid scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset': 'market1501_preprocessed',\n",
    "    'k_shot': 16,\n",
    "    'data_root': '/content/drive/MyDrive/reid_data',\n",
    "    'config_dir': str(project_root / 'config'),\n",
    "\n",
    "    'num_iterations': 1,\n",
    "    'epochs_per_iteration': [40],\n",
    "    'dbscan_eps': 1.5,\n",
    "    'dbscan_eps_decay': 1.0,\n",
    "    'dbscan_min_samples': 8,\n",
    "    'lr_multiplier': [0.1],\n",
    "    'warmup_epochs': 10,\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BASE_LR = 0.00035\n",
    "WEIGHT_DECAY = 0.0005\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_ROOT = '/content/drive/MyDrive/reid_models/ssl+reranking/'\n",
    "\n",
    "print(\"SSL Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[1/3] Loading FULL dataset (all training data)...\")\n",
    "dataloaders = get_dataloaders_from_config(\n",
    "    root=CONFIG['data_root'],\n",
    "    dataset_name=CONFIG['dataset'],\n",
    "    config_dir=str(project_root / 'config'),\n",
    "    k_shot=99,  # Full dataset\n",
    "    override_params={\n",
    "        'height': 256,\n",
    "        'width': 128,\n",
    "        'batch_size_train': BATCH_SIZE,\n",
    "        'batch_size_test': 128,\n",
    "    }\n",
    ")\n",
    "\n",
    "train_loader_full = dataloaders['train']\n",
    "query_loader = dataloaders['query']\n",
    "gallery_loader = dataloaders['gallery']\n",
    "\n",
    "print(f\"  Training samples: {len(train_loader_full.dataset)}\")\n",
    "print(f\"  Query: {len(query_loader.dataset)}, Gallery: {len(gallery_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/3] Collecting all training data...\")\n",
    "all_data = []\n",
    "for batch in tqdm(train_loader_full, desc='Loading data'):\n",
    "    imgs, pids, camids, img_paths = batch\n",
    "    for i in range(len(img_paths)):\n",
    "        all_data.append({\n",
    "            'img_path': img_paths[i],\n",
    "            'pid': pids[i].item(),\n",
    "            'camid': camids[i].item()\n",
    "        })\n",
    "\n",
    "print(f\"  Total samples: {len(all_data)}\")\n",
    "print(f\"  Ground-truth IDs: {len(set([d['pid'] for d in all_data]))} (for reference only)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_results_history = []\n",
    "best_mAP = 0.0\n",
    "best_iteration = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for ssl_iter in range(1, CONFIG['num_iterations'] + 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SSL ITERATION {ssl_iter}/{CONFIG['num_iterations']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\n[Step 1/4] Extracting features from all training data...\")\n",
    "\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    feature_indices = [] \n",
    "\n",
    "    temp_dataset = train_loader_full.dataset\n",
    "    temp_loader = torch.utils.data.DataLoader(\n",
    "        temp_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    sample_idx = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(temp_loader, desc='Extracting features'):\n",
    "            imgs, pids, camids, img_paths = batch\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                features = outputs[1] if len(outputs) > 1 else outputs[0]\n",
    "            else:\n",
    "                features = outputs\n",
    "\n",
    "            features = F.normalize(features, p=2, dim=1)\n",
    "            all_features.append(features.cpu())\n",
    "\n",
    "            batch_size = imgs.size(0)\n",
    "            feature_indices.extend(range(sample_idx, sample_idx + batch_size))\n",
    "            sample_idx += batch_size\n",
    "\n",
    "    all_features = torch.cat(all_features, dim=0).numpy()\n",
    "    print(f\"  Extracted features shape: {all_features.shape}\")\n",
    "    print(f\"  Number of samples in all_data: {len(all_data)}\")  \n",
    "    print(f\"  Feature indices range: {min(feature_indices)} to {max(feature_indices)}\") \n",
    "\n",
    "\n",
    "    print(f\"\\n[Step 2/4] Generating pseudo-labels with DBSCAN...\")\n",
    "\n",
    "    current_eps = CONFIG['dbscan_eps'] * (CONFIG['dbscan_eps_decay'] ** (ssl_iter - 1))\n",
    "    print(f\"  DBSCAN eps={current_eps:.3f}, min_samples={CONFIG['dbscan_min_samples']}\")\n",
    "\n",
    "    print(\"  [1/2] Computing distance matrix...\")\n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    distmat = pairwise_distances(all_features, metric='euclidean')\n",
    "\n",
    "    print(\"  [2/2] Running DBSCAN clustering...\")\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clusterer = DBSCAN(\n",
    "        eps=current_eps,\n",
    "        min_samples=CONFIG['dbscan_min_samples'],\n",
    "        metric='precomputed',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    cluster_labels = clusterer.fit_predict(distmat)\n",
    "\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "    print(f\"\\n  Clustering results:\")\n",
    "    print(f\"    Clusters found: {n_clusters}\")\n",
    "    print(f\"    Noise samples: {n_noise}/{len(cluster_labels)} ({100*n_noise/len(cluster_labels):.1f}%)\")\n",
    "\n",
    "    if n_clusters < 50:\n",
    "        print(f\"    WARNING: Very few clusters! Consider increasing eps to {current_eps*1.2:.3f}\")\n",
    "        if n_clusters == 1:\n",
    "            print(f\"    ERROR: Only 1 cluster! Stopping SSL - features are too similar\")\n",
    "            print(f\"    This usually means the model hasn't learned discriminative features yet\")\n",
    "            break\n",
    "    elif n_clusters > 2000:\n",
    "        print(f\"    WARNING: Too many clusters! Consider decreasing eps to {current_eps*0.8:.3f}\")\n",
    "\n",
    "    cluster_sizes = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_size = (cluster_labels == cluster_id).sum()\n",
    "        cluster_sizes.append(cluster_size)\n",
    "\n",
    "    if cluster_sizes:\n",
    "        print(f\"    Largest cluster: {max(cluster_sizes)} samples\")\n",
    "        print(f\"    Mean cluster size: {np.mean(cluster_sizes):.1f}\")\n",
    "        print(f\"    Median cluster size: {np.median(cluster_sizes):.1f}\")\n",
    "\n",
    "    print(f\"\\n[Step 3/4] Creating SSL training dataset...\")\n",
    "\n",
    "    valid_mask = cluster_labels >= 0\n",
    "    valid_feature_indices = np.where(valid_mask)[0]  \n",
    "\n",
    "    print(f\"  Valid samples (non-noise): {len(valid_feature_indices)}/{len(cluster_labels)}\")\n",
    "\n",
    "    unique_clusters = sorted(set(cluster_labels[valid_mask]))\n",
    "    cluster_to_label = {cluster_id: new_label for new_label, cluster_id in enumerate(unique_clusters)}\n",
    "\n",
    "    ssl_dataset_info = []\n",
    "    for feat_idx in valid_feature_indices: \n",
    "        data_idx = feature_indices[feat_idx]\n",
    "\n",
    "        if data_idx >= len(all_data):\n",
    "            print(f\"    WARNING: Skipping invalid index {data_idx} (max: {len(all_data)-1})\")\n",
    "            continue\n",
    "\n",
    "        cluster_id = cluster_labels[feat_idx]\n",
    "        pseudo_label = cluster_to_label[cluster_id]\n",
    "\n",
    "        ssl_dataset_info.append({\n",
    "            'img_path': all_data[data_idx]['img_path'],\n",
    "            'pseudo_pid': pseudo_label,\n",
    "            'camid': all_data[data_idx]['camid'],\n",
    "            'gt_pid': all_data[data_idx]['pid'], \n",
    "        })\n",
    "\n",
    "    num_pseudo_classes = len(unique_clusters)\n",
    "\n",
    "    print(f\"\\n  SSL Dataset created:\")\n",
    "    print(f\"    Total samples: {len(ssl_dataset_info)}\")\n",
    "    print(f\"    Pseudo-label classes: {num_pseudo_classes}\")\n",
    "    print(f\"    Filtered noise: {n_noise}\")\n",
    "\n",
    "    print(f\"\\n[Step 4/4] Setting up SSL training...\")\n",
    "\n",
    "    print(f\"  Rebuilding model with {num_pseudo_classes} pseudo-classes...\")\n",
    "\n",
    "    new_model = transreid_base(num_classes=num_pseudo_classes, loss='softmax', pretrained=(ssl_iter == 1))\n",
    "\n",
    "    if ssl_iter > 1:\n",
    "        print(f\"  Transferring backbone weights from iteration {ssl_iter-1}...\")\n",
    "        old_state = model.state_dict()\n",
    "        new_state = new_model.state_dict()\n",
    "\n",
    "        transferred = 0\n",
    "        for name, param in old_state.items():\n",
    "            if 'classifier' not in name and 'bottleneck' not in name and 'fc' not in name:\n",
    "                if name in new_state and param.shape == new_state[name].shape:\n",
    "                    new_state[name] = param\n",
    "                    transferred += 1\n",
    "\n",
    "        new_model.load_state_dict(new_state, strict=False)\n",
    "        print(f\"    Transferred {transferred} layers\")\n",
    "\n",
    "    model = new_model.to(device)\n",
    "\n",
    "    print(f\"  Creating SSL dataloader...\")\n",
    "\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class SSLDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, data_info, transform=None):\n",
    "            self.data_info = data_info\n",
    "            self.transform = transform\n",
    "\n",
    "            # Fallback transform jeÅ›li brak\n",
    "            if self.transform is None:\n",
    "                from torchvision import transforms\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize((256, 128)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data_info)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            info = self.data_info[idx]\n",
    "            img = Image.open(info['img_path']).convert('RGB')\n",
    "            img = self.transform(img) \n",
    "            return img, info['pseudo_pid'], info['camid'], info['img_path']\n",
    "\n",
    "    ssl_dataset = SSLDataset(\n",
    "        ssl_dataset_info,\n",
    "        transform=getattr(train_loader_full.dataset, 'transform', None)\n",
    "    )\n",
    "    ssl_loader = torch.utils.data.DataLoader(\n",
    "        ssl_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  \n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    print(f\"    Batches per epoch: {len(ssl_loader)}\")\n",
    "\n",
    "    lr = BASE_LR * CONFIG['lr_multiplier'][ssl_iter - 1]\n",
    "    num_epochs = CONFIG['epochs_per_iteration'][ssl_iter - 1]\n",
    "    warmup_epochs = CONFIG['warmup_epochs'] if ssl_iter == 1 else 5\n",
    "\n",
    "    print(f\"\\n  Training configuration:\")\n",
    "    print(f\"    Epochs: {num_epochs}\")\n",
    "    print(f\"    Base LR: {lr:.6f}\")\n",
    "    print(f\"    Warmup epochs: {warmup_epochs}\")\n",
    "\n",
    "    backbone_params = []\n",
    "    classifier_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' in name or 'bottleneck' in name or 'fc' in name:\n",
    "            classifier_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': backbone_params, 'lr': lr},\n",
    "        {'params': classifier_params, 'lr': lr * 10}  # Higher LR for classifier\n",
    "    ], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=lr * 0.01\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    # Warmup phase (classifier only)\n",
    "    if warmup_epochs > 0:\n",
    "        print(f\"\\n  [Warm-up] Training classifier for {warmup_epochs} epochs...\")\n",
    "        print(f\"    Backbone: frozen\")\n",
    "        print(f\"    Classifier LR: {lr * 10:.6f}\")\n",
    "\n",
    "        # Freeze backbone\n",
    "        for param in backbone_params:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        warmup_optimizer = torch.optim.Adam(classifier_params, lr=lr*10, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        for warmup_epoch in range(1, warmup_epochs + 1):\n",
    "            model.train()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "\n",
    "            pbar = tqdm(ssl_loader, desc=f'Warmup {warmup_epoch}')\n",
    "            for imgs, pids, camids, _ in pbar:\n",
    "                imgs, pids = imgs.to(device), pids.to(device)\n",
    "\n",
    "                warmup_optimizer.zero_grad()\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits = outputs[0]\n",
    "                else:\n",
    "                    logits = outputs\n",
    "\n",
    "                loss = criterion(logits, pids)\n",
    "                loss.backward()\n",
    "                warmup_optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = logits.max(1)\n",
    "                total += pids.size(0)\n",
    "                correct += predicted.eq(pids).sum().item()\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100.*correct/total:.2f}%'\n",
    "                })\n",
    "\n",
    "            epoch_acc = 100. * correct / total\n",
    "            print(f\"    Warmup {warmup_epoch}: Acc={epoch_acc:.2f}%\")\n",
    "\n",
    "        # Unfreeze backbone\n",
    "        for param in backbone_params:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        print(f\"  Warmup complete!\\n\")\n",
    "\n",
    "    # Main SSL Training\n",
    "    print(f\"  [Main Training] {num_epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(ssl_loader, desc=f'SSL {ssl_iter}.{epoch}')\n",
    "        for imgs, pids, camids, _ in pbar:\n",
    "            imgs, pids = imgs.to(device), pids.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits = outputs[0]\n",
    "            else:\n",
    "                logits = outputs\n",
    "\n",
    "            loss = criterion(logits, pids)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += pids.size(0)\n",
    "            correct += predicted.eq(pids).sum().item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(ssl_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "\n",
    "        print(f\"  SSL {ssl_iter}.{epoch}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"\\n  [Evaluation] SSL Iter {ssl_iter}, Epoch {epoch}:\\n\")\n",
    "\n",
    "            eval_results = evaluate_model(\n",
    "                model, query_loader, gallery_loader, device,\n",
    "                use_reranking=True\n",
    "            )\n",
    "\n",
    "            print(f\"    mAP: {eval_results['mAP']:.2%}\")\n",
    "            print(f\"    Rank-1: {eval_results['rank1']:.2%}\")\n",
    "            print(f\"    Rank-5: {eval_results['rank5']:.2%}\")\n",
    "            print(f\"    Rank-10: {eval_results['rank10']:.2%}\")\n",
    "\n",
    "            # Save best model\n",
    "            if eval_results['mAP'] > best_mAP:\n",
    "                best_mAP = eval_results['mAP']\n",
    "                best_iteration = ssl_iter\n",
    "                best_epoch = epoch\n",
    "\n",
    "                checkpoint_path = os.path.join(MODEL_ROOT, \"transreid_pure_ssl_best.pth\")\n",
    "                torch.save({\n",
    "                    'iteration': ssl_iter,\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'num_classes': num_pseudo_classes,\n",
    "                    'mAP': eval_results['mAP'],\n",
    "                    'rank1': eval_results['rank1'],\n",
    "                    'config': CONFIG,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "                print(f\"\\n    NEW BEST MODEL SAVED!\")\n",
    "                print(f\"    mAP: {best_mAP:.2%}, Iteration: {best_iteration}, Epoch: {best_epoch}\\n\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SSL ITERATION {ssl_iter} COMPLETE\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    final_results = evaluate_model(model, query_loader, gallery_loader, device, use_reranking=True)\n",
    "    ssl_results_history.append({\n",
    "        'iteration': ssl_iter,\n",
    "        'mAP': final_results['mAP'],\n",
    "        'rank1': final_results['rank1'],\n",
    "        'rank5': final_results['rank5'],\n",
    "        'rank10': final_results['rank10'],\n",
    "    })\n",
    "\n",
    "    print(f\"  Final results:\")\n",
    "    print(f\"    mAP: {final_results['mAP']:.2%}\")\n",
    "    print(f\"    Rank-1: {final_results['rank1']:.2%}\")\n",
    "\n",
    "    iter_checkpoint_path = os.path.join(MODEL_ROOT, f\"transreid_pure_ssl_iter{ssl_iter}.pth\")\n",
    "    torch.save({\n",
    "        'iteration': ssl_iter,\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'num_classes': num_pseudo_classes,\n",
    "        'results': final_results,\n",
    "        'config': CONFIG,\n",
    "    }, iter_checkpoint_path)\n",
    "    print(f\"  Saved: {iter_checkpoint_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
