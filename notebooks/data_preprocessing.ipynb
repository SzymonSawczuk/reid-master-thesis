{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Data Preprocessing for ReID Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_header",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## Google Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mount_drive_header",
      "metadata": {
        "id": "mount_drive_header"
      },
      "source": [
        "### 1. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mount_drive",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mount_drive",
        "outputId": "89f5b48d-f381-4125-e90c-2722e4c63876"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "clone_repo_header",
      "metadata": {
        "id": "clone_repo_header"
      },
      "source": [
        "### 2. Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clone_repo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone_repo",
        "outputId": "9b874a1d-ae5e-4a44-aefc-8972fe036d91"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "%cd master-thesis-reid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install_header",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "### 3. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_deps",
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirements_colab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_paths_header",
      "metadata": {
        "id": "setup_paths_header"
      },
      "source": [
        "### 4. Setup Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_paths",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_paths",
        "outputId": "55da4774-034a-4073-f630-b6d5f8c340dc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/master-thesis-reid')\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/reid_data\"\n",
        "MODEL_ROOT = \"/content/drive/MyDrive/reid_models\"\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/reid_results\"\n",
        "\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(MODEL_ROOT, exist_ok=True)\n",
        "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
        "\n",
        "print(\"Setup completed successfully!\")\n",
        "print(f\"Data root: {DATA_ROOT}\")\n",
        "print(f\"Model root: {MODEL_ROOT}\")\n",
        "print(f\"Results root: {RESULTS_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports_header",
      "metadata": {
        "id": "imports_header"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports",
        "outputId": "047f1ed6-fc3e-47d4-94eb-b5203eafae25"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from utils.data_loader import OpenCVAugmentation\n",
        "from utils.config_loader import ConfigLoader\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "augmentation_viz_header",
      "metadata": {
        "id": "augmentation_viz_header"
      },
      "source": [
        "## Visualize Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load_sample_image",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_sample_image",
        "outputId": "008ca551-377e-4b61-a785-084a8c8ec2e3"
      },
      "outputs": [],
      "source": [
        "sample_image_path = f\"{DATA_ROOT}/market1501/bounding_box_train/0002_c1s1_000451_03.jpg\"\n",
        "\n",
        "if os.path.exists(sample_image_path):\n",
        "    img = cv2.imread(sample_image_path)\n",
        "    print(f\"Loaded image: {sample_image_path}\")\n",
        "    print(f\"Image shape: {img.shape}\")\n",
        "else:\n",
        "    print(f\"Sample image not found: {sample_image_path}\")\n",
        "    img = np.random.randint(0, 255, (256, 128, 3), dtype=np.uint8)\n",
        "    print(\"Using dummy image for demonstration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize_augmentations",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "visualize_augmentations",
        "outputId": "f01be960-f44c-40fa-9472-96b22d2f59d2"
      },
      "outputs": [],
      "source": [
        "config_loader = ConfigLoader('config')\n",
        "augment_config = config_loader.get_augmentation_config()\n",
        "\n",
        "print(\"Augmentation config:\")\n",
        "for key, value in augment_config.items():\n",
        "    print(f\"  {key:15s}: {value}\")\n",
        "\n",
        "augmentor = OpenCVAugmentation(augment_config)\n",
        "\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "axes[0].imshow(img_rgb)\n",
        "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "aug_funcs = [\n",
        "    ('Rotation', lambda x: augmentor.random_rotation(x)),\n",
        "    ('Horizontal Flip', lambda x: augmentor.random_horizontal_flip(x, p=1.0)),\n",
        "    ('Brightness', lambda x: augmentor.random_brightness(x)),\n",
        "    ('Contrast', lambda x: augmentor.random_contrast(x)),\n",
        "    ('Saturation', lambda x: augmentor.random_saturation(x)),\n",
        "    ('Hue', lambda x: augmentor.random_hue(x)),\n",
        "    ('Gaussian Noise', lambda x: augmentor.random_gaussian_noise(x, sigma_range=(5, 15))),\n",
        "    ('Gaussian Blur', lambda x: augmentor.random_gaussian_blur(x, p=1.0)),\n",
        "    ('Crop & Resize', lambda x: augmentor.random_crop_and_resize(x)),\n",
        "    ('Combined (All)', lambda x: augmentor(x))\n",
        "]\n",
        "\n",
        "for i, (title, aug_func) in enumerate(aug_funcs, 1):\n",
        "    aug_img = aug_func(img.copy())\n",
        "    aug_img_rgb = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n",
        "    axes[i].imshow(aug_img_rgb)\n",
        "    axes[i].set_title(title, fontsize=11)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "axes[11].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{RESULTS_ROOT}/augmentation_examples.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nVisualization saved to: {RESULTS_ROOT}/augmentation_examples.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preprocessing_pipeline_header",
      "metadata": {
        "id": "preprocessing_pipeline_header"
      },
      "source": [
        "## Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessing_function",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preprocessing_function",
        "outputId": "90d5e3d3-6507-40e3-d6e0-cd948aba9b95"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(\n",
        "    input_dir,\n",
        "    output_dir,\n",
        "    target_height=256,\n",
        "    target_width=128,\n",
        "    apply_augmentation=False,\n",
        "    num_augmentations=1,\n",
        "    augmentor=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Preprocess dataset with resizing and optional augmentation.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Input directory with Market-1501 structure\n",
        "        output_dir: Output directory for preprocessed images\n",
        "        target_height: Target image height (default: 256)\n",
        "        target_width: Target image width (default: 128)\n",
        "        apply_augmentation: Whether to apply augmentations\n",
        "        num_augmentations: Number of augmented versions per image\n",
        "        augmentor: OpenCVAugmentation instance (created if None)\n",
        "    \"\"\"\n",
        "    input_path = Path(input_dir)\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if augmentor is None:\n",
        "        augmentor = OpenCVAugmentation()\n",
        "\n",
        "    image_files = list(input_path.glob('*.jpg')) + list(input_path.glob('*.png'))\n",
        "\n",
        "    if len(image_files) == 0:\n",
        "        print(f\"Warning: No images found in {input_dir}\")\n",
        "        return 0\n",
        "\n",
        "    print(f\"Processing {len(image_files)} images...\")\n",
        "\n",
        "    processed_count = 0\n",
        "    for img_path in tqdm(image_files):\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not read {img_path}\")\n",
        "            continue\n",
        "\n",
        "        img_resized = cv2.resize(img, (target_width, target_height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        output_file = output_path / img_path.name\n",
        "        cv2.imwrite(str(output_file), img_resized)\n",
        "        processed_count += 1\n",
        "\n",
        "        if apply_augmentation:\n",
        "            for i in range(num_augmentations):\n",
        "                aug_img = augmentor(img_resized)\n",
        "                aug_filename = output_path / f\"{img_path.stem}_aug{i}{img_path.suffix}\"\n",
        "                cv2.imwrite(str(aug_filename), aug_img)\n",
        "                processed_count += 1\n",
        "\n",
        "    print(f\"Preprocessing complete: {processed_count} images saved to {output_dir}\")\n",
        "    return processed_count\n",
        "\n",
        "print(\"Preprocessing function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "batch_preprocessing_header",
      "metadata": {
        "id": "batch_preprocessing_header"
      },
      "source": [
        "## Batch Preprocessing for Market-1501\n",
        "\n",
        "Process all splits (train, test, query) and save to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_preprocessing_paths",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_preprocessing_paths",
        "outputId": "ce7aa216-fac3-4e3a-cb63-681ecd03617c"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"market1501\"\n",
        "OUTPUT_DATASET_NAME = \"market1501_preprocessed\"\n",
        "\n",
        "input_train = f\"{DATA_ROOT}/{DATASET_NAME}/bounding_box_train\"\n",
        "input_test = f\"{DATA_ROOT}/{DATASET_NAME}/bounding_box_test\"\n",
        "input_query = f\"{DATA_ROOT}/{DATASET_NAME}/query\"\n",
        "\n",
        "output_train = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/bounding_box_train\"\n",
        "output_test = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/bounding_box_test\"\n",
        "output_query = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/query\"\n",
        "\n",
        "data_config = config_loader.get_data_loading_config()\n",
        "target_height = data_config['height']\n",
        "target_width = data_config['width']\n",
        "\n",
        "print(\"Preprocessing configuration:\")\n",
        "print(f\"  Dataset: {DATASET_NAME}\")\n",
        "print(f\"  Output: {OUTPUT_DATASET_NAME}\")\n",
        "print(f\"  Target size: {target_height}x{target_width}\")\n",
        "print(f\"  Input train: {input_train}\")\n",
        "print(f\"  Output train: {output_train}\")\n",
        "print()\n",
        "\n",
        "for split, path in [(\"train\", input_train), (\"test\", input_test), (\"query\", input_query)]:\n",
        "    if os.path.exists(path):\n",
        "        num_files = len(list(Path(path).glob('*.jpg')))\n",
        "        print(f\" {split:6s}: {num_files:5d} images found in {path}\")\n",
        "    else:\n",
        "        print(f\" {split:6s}: NOT FOUND - {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_preprocessing",
      "metadata": {
        "id": "run_preprocessing"
      },
      "outputs": [],
      "source": [
        "augment_config = config_loader.get_augmentation_config()\n",
        "augmentor = OpenCVAugmentation(augment_config)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Starting Batch Preprocessing\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "total_processed = 0\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Training Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(\n",
        "        input_train,\n",
        "        output_train,\n",
        "        target_height=target_height,\n",
        "        target_width=target_width,\n",
        "        apply_augmentation=False,\n",
        "        augmentor=augmentor\n",
        "    )\n",
        "    total_processed += count\n",
        "else:\n",
        "    print(f\"\\nSkipping training set - directory not found: {input_train}\")\n",
        "\n",
        "if os.path.exists(input_test):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Test Set (Gallery)\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(\n",
        "        input_test,\n",
        "        output_test,\n",
        "        target_height=target_height,\n",
        "        target_width=target_width,\n",
        "        apply_augmentation=False,\n",
        "        augmentor=augmentor\n",
        "    )\n",
        "    total_processed += count\n",
        "else:\n",
        "    print(f\"\\nSkipping test set - directory not found: {input_test}\")\n",
        "\n",
        "if os.path.exists(input_query):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Query Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(\n",
        "        input_query,\n",
        "        output_query,\n",
        "        target_height=target_height,\n",
        "        target_width=target_width,\n",
        "        apply_augmentation=False,\n",
        "        augmentor=augmentor\n",
        "    )\n",
        "    total_processed += count\n",
        "else:\n",
        "    print(f\"\\nSkipping query set - directory not found: {input_query}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Preprocessing Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total images processed: {total_processed}\")\n",
        "print(f\"Output directory: {DATA_ROOT}/{OUTPUT_DATASET_NAME}\")\n",
        "print(f\"Target size: {target_height}x{target_width}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify_header",
      "metadata": {
        "id": "verify_header"
      },
      "source": [
        "## Verify Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify_preprocessing",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verify_preprocessing",
        "outputId": "4c0c1f46-e7ca-453f-fc43-46907479880c"
      },
      "outputs": [],
      "source": [
        "print(\"Verification of preprocessed dataset:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for split_name, split_path in [(\"Train\", output_train), (\"Test\", output_test), (\"Query\", output_query)]:\n",
        "    if os.path.exists(split_path):\n",
        "        images = list(Path(split_path).glob('*.jpg'))\n",
        "        print(f\"\\n{split_name} set: {len(images)} images\")\n",
        "\n",
        "        if images:\n",
        "            sample = cv2.imread(str(images[0]))\n",
        "            if sample is not None:\n",
        "                h, w = sample.shape[:2]\n",
        "                print(f\"  Sample image size: {h}x{w}\")\n",
        "                if h == target_height and w == target_width:\n",
        "                    print(f\" Size is correct ({target_height}x{target_width})\")\n",
        "                else:\n",
        "                    print(f\" Size mismatch! Expected {target_height}x{target_width}\")\n",
        "    else:\n",
        "        print(f\"\\n{split_name} set: NOT FOUND\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Verification complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "optional_augmentation_header",
      "metadata": {
        "id": "optional_augmentation_header"
      },
      "source": [
        "<h2>Create Augmented Dataset (save it on disk instead of on-the-fly)</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_augmented_dataset",
      "metadata": {
        "id": "create_augmented_dataset"
      },
      "outputs": [],
      "source": [
        "OUTPUT_AUGMENTED = f\"{DATA_ROOT}/market1501_augmented\"\n",
        "num_augmentations_per_image = 3\n",
        "\n",
        "print(\"Creating augmented dataset...\")\n",
        "print(f\"Each image will have {num_augmentations_per_image} augmented versions\")\n",
        "print()\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"Augmenting training set...\")\n",
        "    preprocess_dataset(\n",
        "        input_train,\n",
        "        f\"{OUTPUT_AUGMENTED}/bounding_box_train\",\n",
        "        target_height=target_height,\n",
        "        target_width=target_width,\n",
        "        apply_augmentation=True,\n",
        "        num_augmentations=num_augmentations_per_image,\n",
        "        augmentor=augmentor\n",
        "    )\n",
        "\n",
        "print(\"Augmented dataset created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lpl57zwmipf",
      "metadata": {
        "id": "lpl57zwmipf"
      },
      "source": [
        "# DukeMTMC-reID Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fnbd03x0vz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "fnbd03x0vz",
        "outputId": "25ef8a23-0226-4c95-c86a-727ff363332c"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"DukeMTMC-reID\"\n",
        "OUTPUT_DATASET_NAME = \"DukeMTMC-reID_preprocessed\"\n",
        "\n",
        "input_train = f\"{DATA_ROOT}/{DATASET_NAME}/bounding_box_train\"\n",
        "input_test = f\"{DATA_ROOT}/{DATASET_NAME}/bounding_box_test\"\n",
        "input_query = f\"{DATA_ROOT}/{DATASET_NAME}/query\"\n",
        "\n",
        "output_train = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/bounding_box_train\"\n",
        "output_test = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/bounding_box_test\"\n",
        "output_query = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/query\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"Preprocessing {DATASET_NAME}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Target size: {target_height}x{target_width}\")\n",
        "print()\n",
        "\n",
        "for split, path in [(\"train\", input_train), (\"test\", input_test), (\"query\", input_query)]:\n",
        "    if os.path.exists(path):\n",
        "        num_files = len(list(Path(path).glob('*.jpg')))\n",
        "        print(f\"{split:6s}: {num_files:5d} images found\")\n",
        "    else:\n",
        "        print(f\"{split:6s}: NOT FOUND - {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k1n9ruv115p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1n9ruv115p",
        "outputId": "55cfe309-2850-4902-8d8f-8defe8731cf5"
      },
      "outputs": [],
      "source": [
        "total_processed = 0\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Training Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_train, output_train, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "if os.path.exists(input_test):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Test Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_test, output_test, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "if os.path.exists(input_query):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Query Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_query, output_query, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DukeMTMC-reID Preprocessing Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total images processed: {total_processed}\")\n",
        "print(f\"Output directory: {DATA_ROOT}/{OUTPUT_DATASET_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0hjp6vefh6o",
      "metadata": {
        "id": "0hjp6vefh6o"
      },
      "source": [
        "### DukeMTMC-reID: Create Augmented Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pinl8j167vf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pinl8j167vf",
        "outputId": "1029b19c-73f0-4181-a582-74ea211982b3"
      },
      "outputs": [],
      "source": [
        "OUTPUT_AUGMENTED = f\"{DATA_ROOT}/DukeMTMC-reID_augmented\"\n",
        "num_augmentations_per_image = 3\n",
        "\n",
        "print(\"Creating augmented dataset for DukeMTMC-reID...\")\n",
        "print(f\"Each image will have {num_augmentations_per_image} augmented versions\")\n",
        "print()\n",
        "\n",
        "DATASET_NAME = \"DukeMTMC-reID\"\n",
        "input_train = f\"{DATA_ROOT}/{DATASET_NAME}/bounding_box_train\"\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"Augmenting training set...\")\n",
        "    preprocess_dataset(\n",
        "        input_train,\n",
        "        f\"{OUTPUT_AUGMENTED}/bounding_box_train\",\n",
        "        target_height=target_height,\n",
        "        target_width=target_width,\n",
        "        apply_augmentation=True,\n",
        "        num_augmentations=num_augmentations_per_image,\n",
        "        augmentor=augmentor\n",
        "    )\n",
        "\n",
        "print(\"Augmented dataset created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742w8xuc8lt",
      "metadata": {
        "id": "742w8xuc8lt"
      },
      "source": [
        "# VeRi-776 Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58asy3wbavc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58asy3wbavc",
        "outputId": "449d4b4d-6c8f-464b-db7a-880d99b8a01e"
      },
      "outputs": [],
      "source": [
        "\n",
        "DATASET_NAME = \"VeRi-776\"\n",
        "OUTPUT_DATASET_NAME = \"VeRi-776_preprocessed\"\n",
        "\n",
        "input_train = f\"{DATA_ROOT}/{DATASET_NAME}/image_train\"\n",
        "input_test = f\"{DATA_ROOT}/{DATASET_NAME}/image_test\"\n",
        "input_query = f\"{DATA_ROOT}/{DATASET_NAME}/image_query\"\n",
        "\n",
        "output_train = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/image_train\"\n",
        "output_test = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/image_test\"\n",
        "output_query = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/image_query\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"Preprocessing {DATASET_NAME} (Vehicle ReID)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Target size: {target_height}x{target_width}\")\n",
        "print()\n",
        "\n",
        "for split, path in [(\"train\", input_train), (\"test\", input_test), (\"query\", input_query)]:\n",
        "    if os.path.exists(path):\n",
        "        num_files = len(list(Path(path).glob('*.jpg')))\n",
        "        print(f\"{split:6s}: {num_files:5d} images found\")\n",
        "    else:\n",
        "        print(f\"{split:6s}: NOT FOUND - {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mba2drfxeq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mba2drfxeq",
        "outputId": "9cc5c711-284a-4513-e621-bc315373cc0d"
      },
      "outputs": [],
      "source": [
        "total_processed = 0\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Training Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_train, output_train, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "if os.path.exists(input_test):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Test Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_test, output_test, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "if os.path.exists(input_query):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Query Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_query, output_query, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VeRi-776 Preprocessing Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total images processed: {total_processed}\")\n",
        "print(f\"Output directory: {DATA_ROOT}/{OUTPUT_DATASET_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oeu0bkbccw",
      "metadata": {
        "id": "oeu0bkbccw"
      },
      "source": [
        "### VeRi-776: Create Augmented Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7mh5u04g6ka",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mh5u04g6ka",
        "outputId": "2aba2494-eecf-4966-d5c7-07a18c21a3fa"
      },
      "outputs": [],
      "source": [
        "OUTPUT_AUGMENTED = f\"{DATA_ROOT}/VeRi-776_augmented\"\n",
        "num_augmentations_per_image = 3\n",
        "\n",
        "print(\"Creating augmented dataset for VeRi-776...\")\n",
        "print(f\"Each image will have {num_augmentations_per_image} augmented versions\")\n",
        "print()\n",
        "\n",
        "DATASET_NAME = \"VeRi-776\"\n",
        "input_train = f\"{DATA_ROOT}/{DATASET_NAME}/image_train\"\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"Augmenting training set...\")\n",
        "    preprocess_dataset(\n",
        "        input_train,\n",
        "        f\"{OUTPUT_AUGMENTED}/image_train\",\n",
        "        target_height=target_height,\n",
        "        target_width=target_width,\n",
        "        apply_augmentation=True,\n",
        "        num_augmentations=num_augmentations_per_image,\n",
        "        augmentor=augmentor\n",
        "    )\n",
        "\n",
        "print(\"Augmented dataset created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2at101bns3h",
      "metadata": {
        "id": "2at101bns3h"
      },
      "source": [
        "# CityFlow Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jrjv5oe4hb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrjv5oe4hb",
        "outputId": "2b44e6ac-7c8d-4ea3-9c12-dc2492c78883"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"CityFlow\"\n",
        "OUTPUT_DATASET_NAME = \"CityFlow_preprocessed\"\n",
        "\n",
        "input_train = f\"{DATA_ROOT}/{DATASET_NAME}/image_train\"\n",
        "input_test = f\"{DATA_ROOT}/{DATASET_NAME}/image_test\"\n",
        "input_query = f\"{DATA_ROOT}/{DATASET_NAME}/image_query\"\n",
        "\n",
        "output_train = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/image_train\"\n",
        "output_test = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/image_test\"\n",
        "output_query = f\"{DATA_ROOT}/{OUTPUT_DATASET_NAME}/image_query\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"Preprocessing {DATASET_NAME} (Vehicle ReID)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Target size: {target_height}x{target_width}\")\n",
        "print()\n",
        "\n",
        "for split, path in [(\"train\", input_train), (\"test\", input_test), (\"query\", input_query)]:\n",
        "    if os.path.exists(path):\n",
        "        num_files = len(list(Path(path).glob('*.jpg')))\n",
        "        print(f\"{split:6s}: {num_files:5d} images found\")\n",
        "    else:\n",
        "        print(f\"{split:6s}: NOT FOUND - {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u2w574vcium",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2w574vcium",
        "outputId": "573d28d2-472c-4e67-aa2b-aff5592e6b25"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "total_processed = 0\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Training Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_train, output_train, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "if os.path.exists(input_test):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Test Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_test, output_test, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "if os.path.exists(input_query):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preprocessing Query Set\")\n",
        "    print(\"=\"*60)\n",
        "    count = preprocess_dataset(input_query, output_query, target_height, target_width, False, 1, augmentor)\n",
        "    total_processed += count\n",
        "\n",
        "# Copy XML label files (required for CityFlow dataset)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Copying XML Label Files\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "source_dataset_root = Path(DATA_ROOT) / DATASET_NAME\n",
        "output_dataset_root = Path(DATA_ROOT) / OUTPUT_DATASET_NAME\n",
        "\n",
        "xml_files = ['train_label.xml', 'test_label.xml', 'query_label.xml']\n",
        "copied_xml = 0\n",
        "\n",
        "for xml_file in xml_files:\n",
        "    source_xml = source_dataset_root / xml_file\n",
        "    dest_xml = output_dataset_root / xml_file\n",
        "\n",
        "    if source_xml.exists():\n",
        "        shutil.copy2(source_xml, dest_xml)\n",
        "        print(f\"Copied: {xml_file}\")\n",
        "        copied_xml += 1\n",
        "    else:\n",
        "        print(f\"Not found: {xml_file}\")\n",
        "\n",
        "print(f\"\\nCopied {copied_xml} XML label files\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CityFlow Preprocessing Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total images processed: {total_processed}\")\n",
        "print(f\"Total XML files copied: {copied_xml}\")\n",
        "print(f\"Output directory: {DATA_ROOT}/{OUTPUT_DATASET_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u2ffoykwye",
      "metadata": {
        "id": "u2ffoykwye"
      },
      "source": [
        "### CityFlow: Create Augmented Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dal9p2ecu4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dal9p2ecu4d",
        "outputId": "5840f479-aa5f-421c-b896-050842bbadfb"
      },
      "outputs": [],
      "source": [
        "OUTPUT_AUGMENTED = f\"{DATA_ROOT}/CityFlow_augmented\"\n",
        "num_augmentations_per_image = 3\n",
        "\n",
        "print(\"Creating augmented dataset for CityFlow...\")\n",
        "print(f\"Each image will have {num_augmentations_per_image} augmented versions\")\n",
        "print()\n",
        "\n",
        "DATASET_NAME = \"CityFlow\"\n",
        "input_train = f\"{DATA_ROOT}/{DATASET_NAME}/image_train\"\n",
        "\n",
        "if os.path.exists(input_train):\n",
        "    print(\"Augmenting training set...\")\n",
        "    preprocess_dataset(\n",
        "        input_train,\n",
        "        f\"{OUTPUT_AUGMENTED}/image_train\",\n",
        "        target_height=target_height,\n",
        "        target_width=target_width,\n",
        "        apply_augmentation=True,\n",
        "        num_augmentations=num_augmentations_per_image,\n",
        "        augmentor=augmentor\n",
        "    )\n",
        "\n",
        "# Copy XML label files to augmented dataset\n",
        "print(\"\\nCopying XML label files to augmented dataset...\")\n",
        "source_dataset_root = Path(DATA_ROOT) / DATASET_NAME\n",
        "output_augmented_root = Path(OUTPUT_AUGMENTED)\n",
        "\n",
        "xml_files = ['train_label.xml', 'test_label.xml', 'query_label.xml']\n",
        "for xml_file in xml_files:\n",
        "    source_xml = source_dataset_root / xml_file\n",
        "    dest_xml = output_augmented_root / xml_file\n",
        "\n",
        "    if source_xml.exists():\n",
        "        shutil.copy2(source_xml, dest_xml)\n",
        "        print(f\"Copied: {xml_file}\")\n",
        "\n",
        "print(\"Augmented dataset created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z78ijchsk6b",
      "metadata": {
        "id": "z78ijchsk6b"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gbiv1jrfp9g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbiv1jrfp9g",
        "outputId": "abf1b8e2-54cb-4801-f53c-5b151e9ef1a5"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "datasets = [\n",
        "    (\"Market-1501\", \"market1501_preprocessed\", [\"bounding_box_train\", \"bounding_box_test\", \"query\"]),\n",
        "    (\"DukeMTMC-reID\", \"DukeMTMC-reID_preprocessed\", [\"bounding_box_train\", \"bounding_box_test\", \"query\"]),\n",
        "    (\"VeRi-776\", \"VeRi-776_preprocessed\", [\"image_train\", \"image_test\", \"image_query\"]),\n",
        "    (\"CityFlow\", \"CityFlow_preprocessed\", [\"image_train\", \"image_test\", \"image_query\"])\n",
        "]\n",
        "\n",
        "for dataset_name, output_name, folders in datasets:\n",
        "    dataset_path = Path(DATA_ROOT) / output_name\n",
        "\n",
        "    if dataset_path.exists():\n",
        "        print(f\"\\n{dataset_name}:\")\n",
        "        print(f\"  Location: {dataset_path}\")\n",
        "\n",
        "        total_images = 0\n",
        "        for folder in folders:\n",
        "            folder_path = dataset_path / folder\n",
        "            if folder_path.exists():\n",
        "                num_images = len(list(folder_path.glob('*.jpg'))) + len(list(folder_path.glob('*.png')))\n",
        "                total_images += num_images\n",
        "                split_name = folder.replace(\"bounding_box_\", \"\").replace(\"image_\", \"\")\n",
        "                print(f\"    {split_name:10s}: {num_images:6d} images\")\n",
        "\n",
        "        print(f\"  {'TOTAL':>12s}: {total_images:6d} images\")\n",
        "    else:\n",
        "        print(f\"\\n{dataset_name}: NOT PREPROCESSED\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"All preprocessed datasets are saved in: {DATA_ROOT}\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "master-thesis-reid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
