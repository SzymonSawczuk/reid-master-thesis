{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "id": "cell-0"
   },
   "source": [
    "# Person Re-Identification Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {
    "id": "cell-1"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-2",
    "outputId": "80ef09c7-d655-4254-9b4e-5d883003208a"
   },
   "outputs": [],
   "source": [
    "import google.colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-4",
    "outputId": "53ff0fcb-cb45-483e-b9fb-a0cad14aad64"
   },
   "outputs": [],
   "source": [
    "%cd master-thesis-reid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e7c17",
   "metadata": {
    "id": "500e7c17"
   },
   "outputs": [],
   "source": [
    "!pip install -q -r requirements_colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2ec39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ce2ec39",
    "outputId": "d0fe06a0-7ee0-4b94-cd8f-32cd3d839711"
   },
   "outputs": [],
   "source": [
    "!pip install torchreid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {
    "id": "cell-5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-6",
    "outputId": "2b4081b2-8813-4084-9d50-ee2d1bd1e4ae"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "project_root = Path('/content/master-thesis-reid')\n",
    "DATA_ROOT = \"/content/drive/MyDrive/reid_data\"\n",
    "MODEL_ROOT = \"/content/drive/MyDrive/reid_models\"\n",
    "RESULTS_ROOT = \"/content/drive/MyDrive/reid_results\"\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "os.makedirs(MODEL_ROOT, exist_ok=True)\n",
    "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Model root: {MODEL_ROOT}\")\n",
    "print(f\"Results root: {RESULTS_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R4vJ-hksE-Gr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4vJ-hksE-Gr",
    "outputId": "99e7bc9c-69e7-4841-e44c-66130b5f9d95"
   },
   "outputs": [],
   "source": [
    "!pip install torchreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-7",
    "outputId": "41f770f4-b120-49a9-85af-1d6457ce1a90"
   },
   "outputs": [],
   "source": [
    "from utils.data_loader import get_dataloaders_from_config\n",
    "from models.person import *\n",
    "import torchreid\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "id": "cell-8"
   },
   "source": [
    "## 3. Load YAML Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-9",
    "outputId": "99b53f1e-ae88-426d-ccdd-4f97f54d8f3c"
   },
   "outputs": [],
   "source": [
    "config_dir = project_root / 'config'\n",
    "\n",
    "\n",
    "with open(config_dir / 'train_experiments.yaml') as f:\n",
    "    experiments_config = yaml.safe_load(f)\n",
    "    print(\"Loaded train_experiments.yaml\")\n",
    "\n",
    "with open(config_dir / 'datasets.yaml') as f:\n",
    "    dataset_config = yaml.safe_load(f)\n",
    "    print(\"Loaded datasets.yaml\")\n",
    "\n",
    "print(\"\\nAvailable person ReID models:\")\n",
    "for model_name in experiments_config['person_reid_experiments'].keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "print(\"\\nAvailable vehicle ReID models:\")\n",
    "for model_name in experiments_config['vehicle_reid_experiments'].keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "print(\"\\nBatch experiment configurations:\")\n",
    "for model_name, config in experiments_config['batch_experiments'].items():\n",
    "    if model_name != 'global_settings' and config.get('enabled'):\n",
    "        print(f\"  - {model_name}: datasets={config['datasets']}, k_shots={config['k_shots']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "id": "cell-10"
   },
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "**Edit this cell to configure your training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-11",
    "outputId": "5b3ec00e-ba11-4642-95bf-8992067d0065"
   },
   "outputs": [],
   "source": [
    "RUN_MODE = 'batch'  \n",
    "\n",
    "if RUN_MODE == 'manual':\n",
    "    \n",
    "    MODELS = ['pcb_p6']\n",
    "    DATASETS = ['market1501']\n",
    "    K_SHOTS = [16]\n",
    "    DATA_TYPES = ['preprocessed']\n",
    "    LOSSES = ['softmax']\n",
    "else:\n",
    "    \n",
    "    \n",
    "    MODELS = []\n",
    "    DATASETS = []\n",
    "    K_SHOTS = []\n",
    "    DATA_TYPES = []\n",
    "    LOSSES = []\n",
    "\n",
    "all_configs = []\n",
    "\n",
    "if RUN_MODE == 'batch':\n",
    "    \n",
    "    batch_config = experiments_config['batch_experiments']\n",
    "    \n",
    "    for model_name, model_batch_config in batch_config.items():\n",
    "        if model_name == 'global_settings':\n",
    "            continue\n",
    "            \n",
    "        if not model_batch_config.get('enabled', True):\n",
    "            print(f\"Skipping {model_name} (disabled)\")\n",
    "            continue\n",
    "        \n",
    "        if model_name in experiments_config['person_reid_experiments']:\n",
    "            base_config = experiments_config['person_reid_experiments'][model_name]\n",
    "            reid_type = 'person'\n",
    "        elif model_name in experiments_config['vehicle_reid_experiments']:\n",
    "            base_config = experiments_config['vehicle_reid_experiments'][model_name]\n",
    "            reid_type = 'vehicle'\n",
    "        else:\n",
    "            print(f\"Warning: {model_name} not found in experiments config\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        for dataset in model_batch_config['datasets']:\n",
    "            for k_shot in model_batch_config['k_shots']:\n",
    "                for data_type in model_batch_config['data_types']:\n",
    "                    for loss in model_batch_config['losses']:\n",
    "                        \n",
    "                        config = {\n",
    "                            'model': model_name,\n",
    "                            'dataset': dataset,\n",
    "                            'data_type': data_type,\n",
    "                            'k_shot': k_shot,\n",
    "                            'loss': loss,\n",
    "                            'reid_type': reid_type,\n",
    "                            \n",
    "                            \n",
    "                            'num_epochs': base_config['training']['num_epochs'],\n",
    "                            'batch_size': base_config['training']['batch_size'],\n",
    "                            'learning_rate': base_config['training']['learning_rate'],\n",
    "                            'weight_decay': base_config['training']['weight_decay'],\n",
    "                            'optimizer': base_config['training']['optimizer'],\n",
    "                            'lr_scheduler': base_config['training']['lr_scheduler'],\n",
    "                            'warmup_epochs': base_config['training'].get('warmup_epochs', 10),\n",
    "                            'lr_milestones': base_config['training'].get('lr_milestones', [40, 60]),\n",
    "                            'lr_gamma': base_config['training'].get('lr_gamma', 0.1),\n",
    "                            'momentum': base_config['training'].get('momentum', 0.9),\n",
    "                            'mixed_precision': base_config['training'].get('mixed_precision', True),\n",
    "                            'save_freq': base_config['training'].get('save_freq', 10),\n",
    "                            \n",
    "                            \n",
    "                            'img_height': base_config['training'].get('img_height', 256),\n",
    "                            'img_width': base_config['training'].get('img_width', 128),\n",
    "                            \n",
    "                            \n",
    "                            'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        }\n",
    "                        \n",
    "                        \n",
    "                        dataset_suffix = {\n",
    "                            'preprocessed': '_preprocessed',\n",
    "                            'augmented': '_augmented',\n",
    "                            'original': ''\n",
    "                        }[data_type]\n",
    "                        \n",
    "                        config['dataset_path'] = os.path.join(\n",
    "                            experiments_config['paths']['data_root'],\n",
    "                            dataset + dataset_suffix\n",
    "                        )\n",
    "                        \n",
    "                        \n",
    "                        model_save_name = f\"{model_name}_{dataset}_{data_type}_k{k_shot}_l{loss}\"\n",
    "                        config['model_save_dir'] = os.path.join(\n",
    "                            experiments_config['paths']['model_root'],\n",
    "                            model_save_name\n",
    "                        )\n",
    "                        os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "                        \n",
    "                        all_configs.append(config)\n",
    "\n",
    "else:\n",
    "    for model in MODELS:\n",
    "        for dataset in DATASETS:\n",
    "            for k_shot in K_SHOTS:\n",
    "                for data_type in DATA_TYPES:\n",
    "                    for loss in LOSSES:\n",
    "                        config = {\n",
    "                            'model': model,\n",
    "                            'dataset': dataset,\n",
    "                            'data_type': data_type,\n",
    "                            'k_shot': k_shot,\n",
    "                            'loss': loss,\n",
    "                            'num_epochs': 60,\n",
    "                            'batch_size': 32,\n",
    "                            'learning_rate': 0.00035,\n",
    "                            'weight_decay': 0.0005,\n",
    "                            'optimizer': 'adamw',\n",
    "                            'lr_scheduler': 'warmup_cosine',\n",
    "                            'warmup_epochs': 10,\n",
    "                            'lr_milestones': [40, 60],\n",
    "                            'lr_gamma': 0.1,\n",
    "                            'momentum': 0.9,\n",
    "                            'mixed_precision': True,\n",
    "                            'save_freq': 10,\n",
    "                            'img_height': 256,\n",
    "                            'img_width': 128,\n",
    "                            'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        }\n",
    "                        \n",
    "                        dataset_suffix = {\n",
    "                            'preprocessed': '_preprocessed',\n",
    "                            'augmented': '_augmented',\n",
    "                            'original': ''\n",
    "                        }[data_type]\n",
    "                        \n",
    "                        config['dataset_path'] = os.path.join(DATA_ROOT, dataset + dataset_suffix)\n",
    "                        \n",
    "                        model_save_name = f\"{model}_{dataset}_{data_type}_k{k_shot}_l{loss}\"\n",
    "                        config['model_save_dir'] = os.path.join(MODEL_ROOT, model_save_name)\n",
    "                        os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "                        \n",
    "                        all_configs.append(config)\n",
    "\n",
    "\n",
    "print(f\"\\nTotal number of configurations to run: {len(all_configs)}\")\n",
    "print(\"\\nConfigurations:\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'#':<4} {'Model':<20} {'Dataset':<15} {'K-shot':<8} {'Data Type':<12} {'Loss':<10}\")\n",
    "print(\"=\" * 90)\n",
    "for i, config in enumerate(all_configs, 1):\n",
    "    print(f\"{i:<4} {config['model']:<20} {config['dataset']:<15} {config['k_shot']:<8} {config['data_type']:<12} {config['loss']:<10}\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-12",
    "outputId": "0a6d5c9d-177d-4e3f-ae13-0f84fd68465d"
   },
   "outputs": [],
   "source": [
    "print(\"Checking dataset paths...\")\n",
    "for config in all_configs:\n",
    "    if not os.path.exists(config['dataset_path']):\n",
    "        print(f\"WARNING: Dataset not found at {config['dataset_path']}\")\n",
    "    else:\n",
    "        print(f\"{config['dataset']}: {config['dataset_path']}\")\n",
    "\n",
    "print(\"\\nAll dataset checks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uk1UP-EDl8PC",
   "metadata": {
    "id": "Uk1UP-EDl8PC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c3ad6",
   "metadata": {
    "id": "122c3ad6"
   },
   "outputs": [],
   "source": [
    "def build_model(model_name, num_classes, loss='softmax'):\n",
    "    \"\"\"\n",
    "    Build a person ReID model using wrapper functions from models.person.\n",
    "\n",
    "    Available models:\n",
    "    - 'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25', 'osnet_ibn_x1_0': OSNet variants\n",
    "    - 'pcb_p6', 'pcb_p4': PCB with 6 or 4 parts\n",
    "    - 'hacnn': HACNN\n",
    "    - 'transreid_base', 'transreid_small': TransReID variants\n",
    "    - 'autoreid_plus': Auto-ReID+\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        num_classes: Number of identities for classification\n",
    "        loss: Loss type ('softmax', 'triplet')\n",
    "\n",
    "    Returns:\n",
    "        Model instance\n",
    "    \"\"\"\n",
    "\n",
    "    if loss in ['softmax+triplet', 'triplet+softmax']:\n",
    "        loss = 'softmax'\n",
    "\n",
    "    if model_name == 'osnet_x1_0':\n",
    "        model = osnet_x1_0(num_classes=num_classes, loss=loss, pretrained=True)\n",
    "\n",
    "    elif model_name == 'pcb_p6':\n",
    "        model = pcb_p6(num_classes=num_classes, loss=loss, pretrained=True)\n",
    "    elif model_name == 'pcb_p4':\n",
    "        model = pcb_p4(num_classes=num_classes, loss=loss, pretrained=True)\n",
    "\n",
    "    elif model_name == 'hacnn':\n",
    "        model = hacnn(num_classes=num_classes, loss=loss, pretrained=True)\n",
    "\n",
    "    elif model_name == 'transreid_base' or model_name == 'transreid' or model_name == 'transreid_base_vehicle':\n",
    "        model = transreid_base(num_classes=num_classes, loss=loss, pretrained=True)\n",
    "\n",
    "    elif model_name == 'autoreid_plus' or model_name == 'autoreid':\n",
    "        model = autoreid_plus(num_classes=num_classes, loss=loss)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}. Available models: \"\n",
    "                        \"osnet_x1_0, pcb_p6, pcb_p4, hacnn, transreid_base, autoreid_plus\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8aecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_float32_for_triplet(outputs):\n",
    "    if isinstance(outputs, (tuple, list)):\n",
    "        return tuple(o.float() if torch.is_tensor(o) else o for o in outputs)\n",
    "    elif torch.is_tensor(outputs):\n",
    "        return outputs.float()\n",
    "    else:\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5dd466",
   "metadata": {},
   "source": [
    "<h2>Model-Specific Configuration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TRIPLET_WEIGHTS = {\n",
    "    \n",
    "    'osnet_x1_0': 1.0,\n",
    "    'osnet_x0_75': 1.0,\n",
    "    'osnet_x0_5': 1.0,\n",
    "    'osnet_x0_25': 1.0,\n",
    "    'osnet_ibn_x1_0': 1.0,\n",
    "    'pcb_p6': 1.0,\n",
    "    'pcb_p4': 1.0,\n",
    "\n",
    "    \n",
    "    'hacnn': 0.5,\n",
    "    'aaver': 0.5,\n",
    "    'rptm': 0.5,\n",
    "\n",
    "    \n",
    "    'transreid_base': 0.3,\n",
    "    'transreid_small': 0.3,\n",
    "\n",
    "    \n",
    "    'autoreid_plus': 0.5,\n",
    "    'autoreid': 0.5,\n",
    "\n",
    "    \n",
    "    'vat': 0.3,\n",
    "\n",
    "    \n",
    "    'resnet50_vehicle': 1.0,\n",
    "}\n",
    "\n",
    "def get_triplet_weight(model_name):\n",
    "    \"\"\"Get appropriate triplet loss weight for model\"\"\"\n",
    "    return MODEL_TRIPLET_WEIGHTS.get(model_name, 1.0)\n",
    "\n",
    "print(\"Model-specific triplet weights loaded\")\n",
    "print(f\"TransReID lambda_tri: {get_triplet_weight('transreid_base')}\")\n",
    "print(f\"OSNet lambda_tri: {get_triplet_weight('osnet_x1_0')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fe4dd",
   "metadata": {},
   "source": [
    "<h2>Helper functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single_output(outputs, pids, loss_type, criterion_ce,\n",
    "                               criterion_triplet, lambda_tri):\n",
    "    \"\"\"\n",
    "    Compute loss for models with single output (OSNet, TransReID with softmax)\n",
    "\n",
    "    Returns: (loss, logits, features)\n",
    "    \"\"\"\n",
    "    if loss_type == 'softmax':\n",
    "        return criterion_ce(outputs, pids), outputs, None\n",
    "\n",
    "    elif loss_type == 'triplet':\n",
    "        outputs_f32 = ensure_float32_for_triplet(outputs)\n",
    "        return criterion_triplet(outputs_f32, pids), None, outputs\n",
    "\n",
    "    else:  # 'softmax+triplet'\n",
    "        outputs_f32 = ensure_float32_for_triplet(outputs)\n",
    "        loss_ce = criterion_ce(outputs, pids)\n",
    "        loss_tri = criterion_triplet(outputs_f32, pids)\n",
    "        loss = loss_ce + lambda_tri * loss_tri\n",
    "        return loss, outputs, outputs\n",
    "    \n",
    "def compute_loss_deep_supervision(outputs, pids, loss_type, criterion_ce,\n",
    "                                 criterion_triplet, lambda_tri):\n",
    "    \"\"\"\n",
    "    Compute loss for DeepSupervision models (PCB, HACNN, AutoReID+, RPTM, AAVER)\n",
    "\n",
    "    Returns: (loss, logits, features)\n",
    "    \"\"\"\n",
    "    if loss_type == 'softmax':\n",
    "        loss = torchreid.losses.DeepSupervision(criterion_ce, outputs, pids)\n",
    "        return loss, outputs[0], None\n",
    "\n",
    "    elif loss_type == 'triplet':\n",
    "        outputs_f32 = ensure_float32_for_triplet(outputs)\n",
    "        loss = torchreid.losses.DeepSupervision(criterion_triplet, outputs_f32, pids)\n",
    "        return loss, None, outputs[0]\n",
    "\n",
    "    else:  # 'softmax+triplet'\n",
    "        loss_ce = torchreid.losses.DeepSupervision(criterion_ce, outputs, pids)\n",
    "        outputs_f32 = ensure_float32_for_triplet(outputs)\n",
    "        loss_tri = torchreid.losses.DeepSupervision(criterion_triplet, outputs_f32, pids)\n",
    "        loss = loss_ce + lambda_tri * loss_tri\n",
    "        return loss, outputs[0], outputs[0]\n",
    "    \n",
    "def compute_loss_vat(outputs, pids, loss_type, criterion_ce,\n",
    "                    criterion_triplet, lambda_tri):\n",
    "    \"\"\"\n",
    "    Compute loss for VAT model (multi-task: id, color, type)\n",
    "\n",
    "    Returns: (loss, logits, features)\n",
    "    \"\"\"\n",
    "    id_logits = outputs[0]  \n",
    "\n",
    "    if loss_type == 'softmax':\n",
    "        return criterion_ce(id_logits, pids), id_logits, None\n",
    "\n",
    "    elif loss_type == 'triplet':\n",
    "        id_logits_f32 = ensure_float32_for_triplet(id_logits)\n",
    "        return criterion_triplet(id_logits_f32, pids), None, id_logits\n",
    "\n",
    "    else:  # 'softmax+triplet'\n",
    "        id_logits_f32 = ensure_float32_for_triplet(id_logits)\n",
    "        loss_ce = criterion_ce(id_logits, pids)\n",
    "        loss_tri = criterion_triplet(id_logits_f32, pids)\n",
    "        loss = loss_ce + lambda_tri * loss_tri\n",
    "        return loss, id_logits, id_logits\n",
    "    \n",
    "\n",
    "def compute_loss_tuple_standard(outputs, pids, loss_type, criterion_ce,\n",
    "                                criterion_triplet, lambda_tri):\n",
    "    \"\"\"\n",
    "    Compute loss for standard tuple output: (features, logits)\n",
    "\n",
    "    Returns: (loss, logits, features)\n",
    "    \"\"\"\n",
    "    features, logits = outputs[0], outputs[1]\n",
    "\n",
    "    if loss_type == 'softmax':\n",
    "        return criterion_ce(logits, pids), logits, None\n",
    "\n",
    "    elif loss_type == 'triplet':\n",
    "        features_f32 = ensure_float32_for_triplet(features)\n",
    "        return criterion_triplet(features_f32, pids), None, features\n",
    "\n",
    "    else:  # 'softmax+triplet'\n",
    "        features_f32 = ensure_float32_for_triplet(features)\n",
    "        loss_ce = criterion_ce(logits, pids)\n",
    "        loss_tri = criterion_triplet(features_f32, pids)\n",
    "        loss = loss_ce + lambda_tri * loss_tri\n",
    "        return loss, logits, features\n",
    "\n",
    "\n",
    "def compute_loss(outputs, pids, config, criterion_ce, criterion_triplet):\n",
    "    \"\"\"\n",
    "    Unified loss computation for all models\n",
    "\n",
    "    Args:\n",
    "        outputs: Model outputs\n",
    "        pids: Person IDs\n",
    "        config: Training configuration dict\n",
    "        criterion_ce: CrossEntropy loss\n",
    "        criterion_triplet: Triplet loss\n",
    "\n",
    "    Returns:\n",
    "        loss: Total loss\n",
    "        logits: Logits for accuracy (or None)\n",
    "        features: Features for triplet (or None)\n",
    "    \"\"\"\n",
    "    model_name = config['model']\n",
    "    loss_type = config['loss']\n",
    "    lambda_tri = get_triplet_weight(model_name)\n",
    "\n",
    "    \n",
    "    if not isinstance(outputs, (tuple, list)):\n",
    "        return compute_loss_single_output(\n",
    "            outputs, pids, loss_type, criterion_ce, criterion_triplet, lambda_tri\n",
    "        )\n",
    "\n",
    "    \n",
    "    if model_name in ['pcb_p6', 'pcb_p4', 'autoreid_plus', 'hacnn', 'rptm', 'aaver']:\n",
    "        return compute_loss_deep_supervision(\n",
    "            outputs, pids, loss_type, criterion_ce, criterion_triplet, lambda_tri\n",
    "        )\n",
    "\n",
    "    \n",
    "    if model_name == 'vat':\n",
    "        return compute_loss_vat(\n",
    "            outputs, pids, loss_type, criterion_ce, criterion_triplet, lambda_tri\n",
    "        )\n",
    "\n",
    "    \n",
    "    return compute_loss_tuple_standard(\n",
    "        outputs, pids, loss_type, criterion_ce, criterion_triplet, lambda_tri\n",
    "    )\n",
    "    \n",
    "def compute_accuracy(logits, features, pids, loss_type):\n",
    "    \"\"\"\n",
    "    Compute training accuracy\n",
    "\n",
    "    Returns: (correct, total)\n",
    "    \"\"\"\n",
    "    if 'softmax' in loss_type and logits is not None:\n",
    "        _, predicted = logits.max(1)\n",
    "        total = pids.size(0)\n",
    "        correct = predicted.eq(pids).sum().item()\n",
    "        return correct, total\n",
    "\n",
    "    elif loss_type == 'triplet' and features is not None:\n",
    "        \n",
    "        dist_mat = torch.cdist(features, features, p=2)\n",
    "        dist_mat_temp = dist_mat.clone()\n",
    "        dist_mat_temp.fill_diagonal_(float('inf'))\n",
    "        nearest_neighbors = dist_mat_temp.argmin(dim=1)\n",
    "\n",
    "        total = pids.size(0)\n",
    "        correct = (pids[nearest_neighbors] == pids).sum().item()\n",
    "        return correct, total\n",
    "\n",
    "    return 0, 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-13",
    "outputId": "a08861db-4018-410f-8c54-670949cfdea5"
   },
   "outputs": [],
   "source": [
    "for config_idx, CONFIG in enumerate(all_configs, 1):\n",
    "    final_model_path = os.path.join(CONFIG['model_save_dir'], 'final_model.pth')\n",
    "    if os.path.exists(final_model_path):\n",
    "        print(f\"\\n[Skipping] {CONFIG['model']} (already trained)\")\n",
    "        continue\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"CONFIGURATION {config_idx}/{len(all_configs)}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {CONFIG['model']}, Dataset: {CONFIG['dataset']}, \"\n",
    "          f\"K-shot: {CONFIG['k_shot']}, Loss: {CONFIG['loss']}\")\n",
    "\n",
    "    if 'triplet' in CONFIG['loss']:\n",
    "        lambda_tri = get_triplet_weight(CONFIG['model'])\n",
    "        print(f\"Triplet weight (lambda_tri): {lambda_tri}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n[1/7] Loading dataset...\")\n",
    "\n",
    "        img_height, img_width = CONFIG['img_height'], CONFIG['img_width']\n",
    "\n",
    "        dataset_suffix = {\n",
    "            'preprocessed': '_preprocessed',\n",
    "            'augmented': '_augmented',\n",
    "            'original': ''\n",
    "        }[CONFIG['data_type']]\n",
    "\n",
    "        dataloaders = get_dataloaders_from_config(\n",
    "            root=experiments_config['paths']['data_root'],\n",
    "            dataset_name=CONFIG['dataset'] + dataset_suffix,\n",
    "            config_dir=str(project_root / 'config'),\n",
    "            model_name=CONFIG['model'],\n",
    "            model_type=CONFIG['reid_type'],\n",
    "            data_type=CONFIG['data_type'],\n",
    "            k_shot=CONFIG['k_shot']\n",
    "        )\n",
    "\n",
    "        train_loader = dataloaders['train']\n",
    "        query_loader = dataloaders['query']\n",
    "        gallery_loader = dataloaders['gallery']\n",
    "\n",
    "        num_train_pids = train_loader.dataset.num_pids\n",
    "        print(f\"Identities: {num_train_pids}, Train: {len(train_loader.dataset)}, \"\n",
    "              f\"Query: {len(query_loader.dataset)}, Gallery: {len(gallery_loader.dataset)}\")\n",
    "\n",
    "        print(f\"\\n[2/7] Setting up data loader...\")\n",
    "\n",
    "        pid2label = train_loader.dataset.pid2label\n",
    "\n",
    "        def collate_fn_with_mapping(batch):\n",
    "            imgs = torch.stack([item[0] for item in batch])\n",
    "            pids = torch.tensor([pid2label[item[1]] for item in batch], dtype=torch.long)\n",
    "            camids = torch.tensor([item[2] for item in batch], dtype=torch.long)\n",
    "            return imgs, pids, camids\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_loader.dataset,\n",
    "            batch_size=train_loader.batch_size,\n",
    "            sampler=train_loader.sampler,\n",
    "            num_workers=train_loader.num_workers,\n",
    "            pin_memory=train_loader.pin_memory,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn_with_mapping\n",
    "        )\n",
    "\n",
    "        print(f\"\\n[3/7] Building model...\")\n",
    "\n",
    "        model = build_model(CONFIG['model'], num_train_pids, loss=CONFIG['loss'])\n",
    "        model = model.to(CONFIG['device'])\n",
    "\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Parameters: {num_params:,}\")\n",
    "\n",
    "        print(f\"\\n[4/7] Setting up training...\")\n",
    "\n",
    "        criterion_ce = torchreid.losses.CrossEntropyLoss(\n",
    "            num_classes=num_train_pids,\n",
    "            use_gpu=True,\n",
    "            label_smooth=True\n",
    "        )\n",
    "        criterion_triplet = torchreid.losses.TripletLoss(margin=0.3)\n",
    "\n",
    "        if CONFIG['optimizer'] == 'sgd':\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=CONFIG['learning_rate'],\n",
    "                momentum=CONFIG['momentum'],\n",
    "                weight_decay=CONFIG['weight_decay']\n",
    "            )\n",
    "        else:\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=CONFIG['learning_rate'],\n",
    "                weight_decay=CONFIG['weight_decay']\n",
    "            )\n",
    "\n",
    "        if CONFIG['lr_scheduler'] == 'multistep':\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer,\n",
    "                milestones=CONFIG['lr_milestones'],\n",
    "                gamma=CONFIG['lr_gamma']\n",
    "            )\n",
    "        elif CONFIG['lr_scheduler'] == 'warmup_cosine':\n",
    "            from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "            warmup = LinearLR(\n",
    "                optimizer,\n",
    "                start_factor=0.01,\n",
    "                end_factor=1.0,\n",
    "                total_iters=CONFIG['warmup_epochs']\n",
    "            )\n",
    "            cosine = CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=CONFIG['num_epochs'] - CONFIG['warmup_epochs']\n",
    "            )\n",
    "            scheduler = SequentialLR(\n",
    "                optimizer,\n",
    "                schedulers=[warmup, cosine],\n",
    "                milestones=[CONFIG['warmup_epochs']]\n",
    "            )\n",
    "        else:\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "        scaler = torch.amp.GradScaler('cuda')\n",
    "        print(f\"Optimizer: {CONFIG['optimizer']}, LR: {CONFIG['learning_rate']}, \"\n",
    "              f\"Scheduler: {CONFIG['lr_scheduler']}, Epochs: {CONFIG['num_epochs']}\")\n",
    "\n",
    "        print(f\"\\n[5/7] Training model...\")\n",
    "\n",
    "        history = {'train_loss': [], 'train_acc': [], 'learning_rate': []}\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{CONFIG[\"num_epochs\"]}')\n",
    "            for batch_idx, (imgs, pids, camids) in enumerate(pbar):\n",
    "                imgs = imgs.to(CONFIG['device'])\n",
    "                pids = pids.to(CONFIG['device'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    if CONFIG['model'] in ['transreid_base', 'transreid_small']:\n",
    "                        camids_zero_based = camids - 1\n",
    "                        outputs = model(imgs, cam_label=camids_zero_based)\n",
    "                    else:\n",
    "                        outputs = model(imgs)\n",
    "\n",
    "                    loss, logits, features = compute_loss(\n",
    "                        outputs, pids, CONFIG, criterion_ce, criterion_triplet\n",
    "                    )\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                batch_correct, batch_total = compute_accuracy(\n",
    "                    logits, features, pids, CONFIG['loss']\n",
    "                )\n",
    "                correct += batch_correct\n",
    "                total += batch_total\n",
    "\n",
    "                pbar_dict = {'loss': running_loss / (batch_idx + 1)}\n",
    "                if total > 0:\n",
    "                    pbar_dict['acc'] = 100. * correct / total\n",
    "                pbar.set_postfix(pbar_dict)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_acc = 100. * correct / total if total > 0 else 0.0\n",
    "\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            history['train_acc'].append(epoch_acc)\n",
    "            history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # Save best model\n",
    "            if ((total > 0 and epoch_acc > best_acc) or\n",
    "                (total == 0 and (epoch == 1 or epoch_loss < min(history['train_loss'][:-1])))):\n",
    "                best_acc = epoch_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'config': CONFIG,\n",
    "                    'num_classes': num_train_pids,\n",
    "                }, os.path.join(CONFIG['model_save_dir'], 'best_model.pth'))\n",
    "\n",
    "            # Save checkpoint\n",
    "            if epoch % CONFIG['save_freq'] == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'config': CONFIG,\n",
    "                    'num_classes': num_train_pids,\n",
    "                }, os.path.join(CONFIG['model_save_dir'], f\"checkpoint_epoch{epoch}.pth\"))\n",
    "\n",
    "        print(f\"Training completed! Best accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'epoch': CONFIG['num_epochs'],\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': CONFIG,\n",
    "            'num_classes': num_train_pids,\n",
    "            'history': history,\n",
    "            'best_acc': best_acc,\n",
    "        }, os.path.join(CONFIG['model_save_dir'], 'final_model.pth'))\n",
    "\n",
    "        print(f\"\\n[6/7] Extracting features...\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        def extract_features(dataloader):\n",
    "            features_list, pids_list, camids_list = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for imgs, pids, camids, _ in tqdm(dataloader, desc='Extracting', leave=False):\n",
    "                    imgs = imgs.to(CONFIG['device'])\n",
    "                    feats = model(imgs)\n",
    "\n",
    "                    if isinstance(feats, tuple):\n",
    "                        feats = feats[0]\n",
    "\n",
    "                    feats = F.normalize(feats, p=2, dim=1)\n",
    "                    features_list.append(feats.cpu())\n",
    "                    pids_list.append(pids)\n",
    "                    camids_list.append(camids)\n",
    "\n",
    "            return (torch.cat(features_list).numpy(),\n",
    "                    torch.cat(pids_list).numpy(),\n",
    "                    torch.cat(camids_list).numpy())\n",
    "\n",
    "        query_features, query_pids, query_camids = extract_features(query_loader)\n",
    "        gallery_features, gallery_pids, gallery_camids = extract_features(gallery_loader)\n",
    "\n",
    "        sample_img = torch.randn(1, 3, img_height, img_width).to(CONFIG['device'])\n",
    "\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = model(sample_img)\n",
    "\n",
    "        times = []\n",
    "        for _ in range(100):\n",
    "            if CONFIG['device'] == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                _ = model(sample_img)\n",
    "            if CONFIG['device'] == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "\n",
    "        avg_time_ms = np.mean(times) * 1000\n",
    "        fps = 1.0 / np.mean(times)\n",
    "        print(f\"Inference: {avg_time_ms:.2f} ms, {fps:.2f} FPS\")\n",
    "\n",
    "        print(f\"\\n[7/7] Evaluating model...\")\n",
    "\n",
    "        qf = torch.from_numpy(query_features)\n",
    "        gf = torch.from_numpy(gallery_features)\n",
    "\n",
    "        distmat = metrics.compute_distance_matrix(qf, gf, metric='euclidean').numpy()\n",
    "        distmat_qq = metrics.compute_distance_matrix(qf, qf, metric='euclidean').numpy()\n",
    "        distmat_gg = metrics.compute_distance_matrix(gf, gf, metric='euclidean').numpy()\n",
    "        distmat_reranked = torchreid.utils.re_ranking(distmat, distmat_qq, distmat_gg)\n",
    "\n",
    "        cmc, mAP = metrics.evaluate_rank(\n",
    "            distmat_reranked,\n",
    "            query_pids,\n",
    "            gallery_pids,\n",
    "            query_camids,\n",
    "            gallery_camids,\n",
    "            use_metric_cuhk03=False\n",
    "        )\n",
    "\n",
    "        print(f\"mAP: {mAP:.2%}, Rank-1: {cmc[0]:.2%}, \"\n",
    "              f\"Rank-5: {cmc[4]:.2%}, Rank-10: {cmc[9]:.2%}\")\n",
    "\n",
    "        # Save results\n",
    "        results = {\n",
    "            'model': CONFIG['model'],\n",
    "            'dataset': CONFIG['dataset'],\n",
    "            'data_type': CONFIG['data_type'],\n",
    "            'k_shot': CONFIG['k_shot'],\n",
    "            'loss': CONFIG['loss'],\n",
    "            'lambda_tri': get_triplet_weight(CONFIG['model']),\n",
    "            'optimizer': CONFIG['optimizer'],\n",
    "            'learning_rate': CONFIG['learning_rate'],\n",
    "            'num_epochs': CONFIG['num_epochs'],\n",
    "            'mAP': float(mAP),\n",
    "            'rank1': float(cmc[0]),\n",
    "            'rank5': float(cmc[4]),\n",
    "            'rank10': float(cmc[9]),\n",
    "            'rank20': float(cmc[19]),\n",
    "            'best_train_acc': float(best_acc),\n",
    "            'final_train_loss': float(history['train_loss'][-1]),\n",
    "            'avg_query_time_ms': float(avg_time_ms),\n",
    "            'fps': float(fps),\n",
    "            'img_height': img_height,\n",
    "            'img_width': img_width,\n",
    "        }\n",
    "\n",
    "        all_results.append(results)\n",
    "\n",
    "        model_save_name = (f\"{CONFIG['model']}_{CONFIG['dataset']}_\"\n",
    "                          f\"{CONFIG['data_type']}_k{CONFIG['k_shot']}\")\n",
    "\n",
    "        # Save history\n",
    "        with open(os.path.join(RESULTS_ROOT, f\"{model_save_name}_history.json\"), 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "\n",
    "        \n",
    "        with open(os.path.join(RESULTS_ROOT, f\"{model_save_name}_results.json\"), 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        print(f\"\\nConfiguration {config_idx}/{len(all_configs)} completed successfully!\")\n",
    "\n",
    "        \n",
    "        del model, optimizer, scheduler, train_loader, query_loader, gallery_loader\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in configuration {config_idx}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EXPERIMENTS COMPLETED!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "master-thesis-reid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
